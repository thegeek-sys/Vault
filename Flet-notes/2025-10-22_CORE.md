## $\verb|MPI_Bcast|$
Data belonging to a single process is sent to all of the processes in the communicator

```c
int MPI_Bcast(
	void*        data_p,      // in/out
	int          count,       // in
	MPI_Datatype datatype,    // in
	int          source_proc, // in
	MPI_Comm     comm         // in
);
```

data_p: rank 0 invia valore tutti gli altri si trovano i dati ricevuti

## $\verb|MPI_Allreduce|$
Conceptually, an `MPI_Reduce` followed by `MPI_Bcast` (ig. compute a global sum and distribute the result to all the processes)

una reduce e poi una broadcast per distribuire tutti i valori

![[Pasted image 20251022142202.png|300]]

```c
int MPI_Allreduce(
	void*        input_data_p,  // in
	void*        output_data_p, // out
	int          count,         // in
	MPI_Datatype datatype,      // in
	MPI_Op       operator,      // in
	MPI_Comm     comm           // in
);
```

The argument list id identical to that for `MPI_Reduce`, except that there is no `dest_process` since all the processes should get the result

![[Pasted image 20251022142701.png|300]]

(non necessariamente poi la reduce verrà fatta in questo modo, è più efficiente in realtà)

>[!question] Is this the best way of doing it?
>Secondo l’immagine questo è il tempo che impiega
>$2\cdot \log_{2}(p)\cdot T_{\text{send}}$ tempo di send e receive approssimate allo stesso tempo (T send)
>
>![[Pasted image 20251022143012.png]]
>This is also known as butterfly pattern (sometimes as recursive distance doubling)
>
>$T=\log_{2}(p)\cdot T_{\text{send}}$
>
>It is two times faster (other algos might be better depending on the data size)

## Revelance of collective algorithms
Widely used in large-scale parallel applications from many domains. Account for a large fraction of the total runtime and is higly relevant for distributed training of deep-learning models

That’s the reason why all the big players are designing their own collective communication library. For example:
- NCCL (NVIDIA)
- RCCL (AMD)
- OnceCCL (Intel)
- MSCCL (Microsoft)
- …

Give a collective (eg. `MPI_Reduce`), how to select the best algorithm?
- automatically through heuristic
- manually
- MPI implementations such as Open MPI do not make assumption on the underlying hardware 

It is an active research area, both from algorithmic and implementations standpoints