## Can we extrapole expectations? Amdahl’s law
Intuition: each program has some part of it which cannot be parallelized (serial fraction $1-a$)

>[!example]
>Reading/writing a file from disk, sending/receiving data over the network, serialization due to lock/unlock, etc.

Amdahl’s law says that the speedup is limited by the serial fraction ($s$)

(la parte con p può essere parallelizzata la parte con s no, (es. in loop per calcolare elemento corrente devo calcolare quello precedente)) per questo non si può mai ottenere lo speedup ideale
![[Pasted image 20251028171035.png|400]]

Amdahl’s law says that the speedup is limited by the serial fraction
$$
T_{\text{parallel}}(p)=(1-\alpha)T_{\text{serial}}+\frac{\alpha T_{\text{seiral}}}{p}
$$

A fraction $0\leq \alpha\leq 1$ can be parallelized. The remaining $1-a$ has to be done sequentially
alpha è la percentuale parallelizzabile

>[!example]
>- if $\alpha= 0$, the code can’t be parallelized and $T_{\text{parallel}}(p)=T_{\text{serial}}$
>- if $\alpha=1$, the entire code can be parallelized and $T_{\text{parallel}}(p)=\frac{T_{\text{serial}}}{p}$ (ideal speedup)

speedup
$$
S(p)=\frac{T_{\text{serial}}}{(1-\alpha)T_{\text{serial}}+\frac{\alpha T_{\text{serial}}}{p}}
$$

$$
\lim_{ p \to \infty } S(p)=\frac{1}{1-\alpha}
$$

importante perchè dice limite asintotico a cui possiamo tendere con lo speedup

(to fix)
>[!example]
>- if $60\%$ of the application can be parallelized, $\alpha = 0.6$, which means we can expect a speedup of at most $2.5$
>- if $80\%$ of the application can be parallelized, $\alpha = 0.8$, which means we can expect a speedup of at most $5$
>
>To be able to scale up to $100000$ (è lo speedup) processes, we need to have $\alpha >= 0.99999$

![[Pasted image 20251028172251.png]]

## Gustafon’s law
(al posto di considerare lo strong scaling si usa il weak scaling)

If we consider weak scaling, the parallel fraction increases with the problem size (i.e., the serial time remains constant, but the parallel time increases)

It is also known as scaled speedup
$$
S(n,p)=(1-\alpha)+\alpha p
$$

![[Pasted image 20251028172451.png]]

quando andiamo ad analizzare il weak scaling il carico di lavoro rimane costante

## Amdahl’s law limitations
The serial fraction could get bigger when increasing the number of processor (i.e. the runtime might increase when increasing the number of processors)

![[Pasted image 20251028172559.png]]

## A parallell sorting algorithm

>[!info]
>- $n$ keys and $p=$ comm sz processes
>- $n/p$ keys assigned to each process

No restrictions on which keys are assigned to which processes, when the algorithm terminates:
- the keys assigned to each process should be sorted in (say) incresing order
- if $0\leq q<r<p$, then each key assigned to processo $q$ should be less than or equal to every key assigned to process $r$

```c
void Bubble_sort(
	int a[], // in/out
)
```

## Example: sum between vectors
$$
\begin{align}
x+y&=(x_{0},x_{1},\dots,x_{n-1})+(y_{0},y_{1},\dots,y_{n-1}) \\
&=
\end{align}
$$

### Parallel implementation

```c
void Parallel_vector_sum(
) {
	int local_i;
	
	for (local_i=0; local_i<local_n; local_i++)
		local_z[local_i] = local_x[local_i] + local_y[local_i];
}
```

## Scatter
scatter, dato vettore iniziale, il rank 0 lo divide e manda un pezzetto ad ogni processo
`MPI_Scatter` can be used in a fuction that reads in an entire vector on process 0 but only send the needed components to each of the other processes

scatter_v posso gestire le quantità da inviare ai singoli processi, in scatter il resto viene perso

MPI_IN_PLACE per evitare al rank 


## Gather
(nell’out vengono ordinati per rank, gaso)
Collect all of the components of the vector onto process 0, and then process 0 can process all of the components

## Collectives on matrices
(slide 48)
primo metodo sbagliato perchè dopo le prime quattro righe ho cose a caso
secondo sbagliato pk dopo i primi 4 elementi della prima riga ho cose a caso

terzo ok

metodo migliore è di linearizzare la matrice

```c
int* a;
a = (int*) malloc(sizeof(int)*num_rows*num_cols);
…
…
// a[i][j]
a[i * num_cols + j] = ….
```

al posto di fare una matrice bidimensionale ho un’array unidimensionale 

## Example 2: Matrix-vector multiplication
## How to parallelize?
1. broadcast the vector $x$ from rank 0 to the othere processes
2. scatter the rows of the matrix $A$ from rank 0 to the other processes
3. each process computes a subset of the elements of the resulting vector $y$
4. gather the final vector $y$ to rank 0

## Matrix scattering

```c
void Read_matrix(...) {
	double* A = NULL;
	int local_ok = 1;
	int i, j;
	
	if (my_rank == 0) {
		A = malloc(m*n*sizeof(double));
		printf("Enter the matrix %s\n", prompt)
	}
}
```

Now, let’s assume that this is done in a loop, and the output y is used as the
input vector for the next iteration
(vettore risultante con valore di ingresso per l’iterazione successiva)

spesso quando ci stanno due chiamate collettive su due righe consecutive, probabilmente ci sta una collettiva che lo fa

## Reduce scatter
Each rank gets the sum of just a part of the vector

## MPI derived datatypes
(principalmente per le struct)

Used to represent any collection of data items in memory by storing both the types of the items and their relative locations in memory. The idea is that if a function that sends data knows the information about a collection of data items, it cna collect the items from memory before they are sent

Similarly, a function that receives data can distribute the items into their correct destinations in memory when they’re received

Formally, consists of a sequence of basic MPI data types together with a displacement for each of the data types

### $\verb|MPI_Type_create_struct|$
Builds a derived datatype that consists of individual elements that have different basic types

displacement offset relativo all’inizio della struct

```c
int MPI_Type_create_struct(
	int           count,                    // in
	int           array_of_blocklengths[],  // in
	MPI_Aint      array_of_displacements[], // in
	MPI_Datatype  array_of_types[],         // in
	MPI_Datatype* new_type_p                // out
);
```

### $\verb|MPI_Type_commit|$
Allows the MPI implementation to optimize its internal representation of the datatype for use in communication functions.

```c
int MPI_Type_commit(MPI_Datatype* new_mpi_t_p /* in/out */);
```

### $\verb|MPI_Type_free|$
When we’re finished with our new type, this frees any additional storage used.

```c
int MPI_Type_free(MPI_Datatype* old_mpi_t_p /* in/out */);
```