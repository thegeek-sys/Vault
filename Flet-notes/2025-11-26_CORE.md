## Context Switching
Usually a SM has more resident blocks/warps than what it is able to concurrently run. Each SM can switch seamlessly between warps

Each thread has its own private execution context that is maintained on-chip (i.e. context switch comes for free)

When an instraction to be executed by a warp need to wait for the result of a previously initiated long-latency operation, the warp is not selected for execution (e.g. memory read, long latency floating-point operations).

This mechanism of filling the latency time of operations with work from other threads often called “latency tolerance” or “latency hiding”

Given a sufficient number of warps, the hardware will likely find a warp to execute at any point in time, thus making full use of the execution hardware in spite of these long latency operations

With warp scheduling, the long waiting time of warp instruction is “hidden” by executing instrction from other warps

This ability to tolerate long-latency operations is the main reason GPUs do not dedicate nearly as much chip area to cache memories and branch prediction mechanisms as do CPUs

 cambio contesto su GPU non costa tanto. perché ciascun thread (in esecuzione o non) ha il contesto di esecuzione privato, quindi non deve essere spostato. quando un core sta aspettando un dato viene fatto context switch al volo su altro thread così da non perdere tempo (nessun core idle)

### Example 1
A CUDA device allows up to 8 blocks and 1024 threads per SM, and 512 threads per block. Shall we use 8x8, 16x16 or 32x32 thread block?

vogliamo saturare il numero di thread

**8x8 blocks:**
we would have 64 threads per block, so to fill the 1024 threads we can have for each SM, we  would need $1024/64=16$ blocks
However, we can have at most 8 blocks per SM, thus we would end with only $64\times 8=512$ threads per SM. We are not fully utilizing the resources. Most likely, at some time, the scheduled might not find threads to schedule when some thread is waiting for long-latency operations

**16x16**
We would have 256 threads per block, so to fill the 1024 threads we can have for each SM, we would need $1024/256=4$ blocks. This would allow to have 1024 threads on the SM, so there will be a lot of opportunities for latency hiding

**32x32**
We would have 1024 threads per block, which is higher than the 512 threads per block we can have

In practice, it is more complicated than this, the usage of other resources as registers and shared memory must be taken into account

## Example 2
A CUDA device allows up to 8 blocks and 1536 threads per SM, and 1024 threads per block. Shall we use 8x8, 16x16 or 32x32 thread block

24>8
Only 512 threads (8x8x8) would go into each SM (NON SCRIVE NON HA SENSO, PIU SEMPLICE 24>8)

16x16
We would have 256 threads per block, so to fill the 1536 threads we can have for each SM, we would need $1536/256 = 6$ blocks
We achieve full capacity (unless other resources constraints come into play)

## Device properties

>[!example] Listing all the GPUs in a system
```c
int deviceCount = 0;
cudaGetDeviceCount(&deviceCount);
if (deviceCount == 0)
	printf("No CUDA compatible GPU exists\n")
else {
	cudaDeviceProp pr;
	for (int i=0; i<deviceCount; i++) {
		cudaGetDeviceProperties(&pr, i);]
		printf("Dev #%i is %s\n", i, pr.name);
	}
}

struct cudaDeviceProp {
    char name[256];         // A string identifying the device
    int major;              // Compute capability major number
    int minor;              // Compute capability minor number
    int maxGridSize [3];
    int maxThreadsDim [3];
    int maxThreadsPerBlock;
    int maxThreadsPerMultiProcessor;
    int multiProcessorCount;
    int regsPerBlock;       // Number of registers per block
    size_t sharedMemPerBlock;
    size_t totalGlobalMem;
    int warpSize;
    // ... altri campi omessi
};
```