## Caching
A collection of memory locations that can be accessed in less time than some other memory locations. A CPU cache is typically located on the same chip, or one that can be accessed much faster than ordinary memory (it is usually physically closer)

Also user more performing (but also more expensive) technology (e.g. SRAM instead of DRAM), so it is going to be faster but smaller

### What to cache
We assume locality (of both for instructions and data). I.e., accessing one location is followed by an access of a nearby locations

- Spatial locality → accessing a nearby location.
- Temporal locality → accessing it in the near future.

```c
// z[i] follows z[i-1] in memory (spatial locality)
float z[1000];
...
sum = 0.0;
// access to z[i] follows access to z[i-1] (temporal locality)
for (i=0; i<1000; i++)
	z += z[i];
```

quando accediamo a z[0] viene caricato in cache un certo numero di elementi (16)

### Cache lines
Data is transferred from memory to cache in blocks/lines. Doing one transfer of 16 memory locations, is better than doing 16 transfers of one memory location  each

When accessing z[0] you need to wait for the transfer, but then you will find the other 15 elements in cache already

### Cache levels
![[Pasted image 20251209172050.png]]

Data stored in L1 might or might not be stored in L2/L3 as well (it depends on the type of the cache). The CPU first checks if the data is in L1, if not, checks in L2, etc.

>[!question] Why do we care?
>To write efficient/performant parallel code:
>- its sequential parts must be efficient/performant (try to think about how your application accesses the data, random accesses are much worst than linear accesses)
>- the coordination between these sequential parts must be done efficiently

## Consistency
supponiamo che x sia in cache se aggiorno il valore x in cache, comporta che la copia di x è diversa in memoria principale rispetto alla cache. 

When a CPU writes data to cache, the value in cache may be inconsistent with the value in main memory.
modi per risolvere il problema di prima
- write-through → caches handle this by updating the data in main memory at the time it is written to cache
- write-back → caches mark data in the cache as dirty, and when the cache line is replaced by a new cache line from memory, the dirty line is written to memory

## Caching on multicores
### Cache coherence
Programmers have no control over caches and when they get updated