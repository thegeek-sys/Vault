## Caching
A collection of memory locations that can be accessed in less time than some other memory locations. A CPU cache is typically located on the same chip, or one that can be accessed much faster than ordinary memory (it is usually physically closer)

Also user more performing (but also more expensive) technology (e.g. SRAM instead of DRAM), so it is going to be faster but smaller

### What to cache
We assume locality (of both for instructions and data). I.e., accessing one location is followed by an access of a nearby locations

- Spatial locality → accessing a nearby location.
- Temporal locality → accessing it in the near future.

```c
// z[i] follows z[i-1] in memory (spatial locality)
float z[1000];
...
sum = 0.0;
// access to z[i] follows access to z[i-1] (temporal locality)
for (i=0; i<1000; i++)
	z += z[i];
```

quando accediamo a z[0] viene caricato in cache un certo numero di elementi (16)

### Cache lines
Data is transferred from memory to cache in blocks/lines. Doing one transfer of 16 memory locations, is better than doing 16 transfers of one memory location  each

When accessing z[0] you need to wait for the transfer, but then you will find the other 15 elements in cache already

### Cache levels
![[Pasted image 20251209172050.png]]

Data stored in L1 might or might not be stored in L2/L3 as well (it depends on the type of the cache). The CPU first checks if the data is in L1, if not, checks in L2, etc.

>[!question] Why do we care?
>To write efficient/performant parallel code:
>- its sequential parts must be efficient/performant (try to think about how your application accesses the data, random accesses are much worst than linear accesses)
>- the coordination between these sequential parts must be done efficiently

## Consistency
supponiamo che x sia in cache se aggiorno il valore x in cache, comporta che la copia di x è diversa in memoria principale rispetto alla cache. 

When a CPU writes data to cache, the value in cache may be inconsistent with the value in main memory.
modi per risolvere il problema di prima
- write-through → caches handle this by updating the data in main memory at the time it is written to cache
- write-back → caches mark data in the cache as dirty, and when the cache line is replaced by a new cache line from memory, the dirty line is written to memory

## Caching on multicores
### Cache coherence
Programmers have no control over caches and when they get updated

il problema dell’inconsistenza è relativa al fatto che un core ha mantenuto il valore vecchio rispetto all’altro core. dobbiamo fare in modo che la modifica venga propagata all’altro core

ci stanno due tecniche: snooping e …
### Snooping cache coherence
The cores share a bus and any signal transmitted on the bus can be “seen” by all cores connected to the bus. When core 0 updated the copy of `x` stored in its cache it also broadcasts this information across the bus. If core 1 is “snooping” the bus, it will see that `x` has been updated and it can mark its copy of `x` as invalid

>[!warning]
>It’s not used anymore: broadcast is expensive nowadays we have multicores with 64/128 cores

### Directory base cache coherence
Uses a data structure called a directory that stores the status of each cache line (e.g., a bitmap/list saying which cores has a copy of that line) (quando invalido un pezzo di cache, devo invalidare tutta la linea)

When a variable is updated, the directory is consulted, and the cache controllers of the cores that have that variable’s cache line in their caches are invalidated

## False sharing
Data is fetched for memory to cache in lines. Each line can contain several variables (e.g. if a cache is 64 bytes long, it can contain 16 4-byte integers, which were consecutive in memory).
When data is invalidated, the entire line is invalidate. Even if two threads access two different variables, if those are on the same cache line, this would still cause an invalidation

It is called false sharing because we are sharing cache lines without actually sharing data

How to fix it:
- pad the data
- cache the mapping of data to threads/cores
- use private/local variables

>[!done] Solution
>Try to force variables which are accessed by different threads to be on different cache lines. Padding (but be aware, the compiler might change the order of the fields in a struct). To avoid that, do something like (assuming 64 bytes cache lines):
>```c
>struct alignTo64ByteCacheLine {
>	int _onCacheLine1 __attribute__((aligned(64)))
>	int _onCacheLine2 __attribute__((aligned(64)))
>}
>```
>
>>[!question] How to get the cache line size?
>>- from the code → `sysconf(_SC_LEVEL1_DCACHE_LINESIZE)`
>>- from the shell → `getconf LEVEL1_DCACHE_LINESIZE`
>
>Do all the updates on a variable local to the thread (e.g. allocated on the stack, and then write it on heap only at the end)


il problema del padding è che spreca molta memoria, l’altra soluzione è fare gli aggiornamenti su una variabile locali e fare la scrittura sullo heap solo alla fine
## Padding the data
Original:
```c
double x[N];
#pragma omp parallel for schedule(static, 1)
for (int i=0; i<N; i++)
	x[i] = someFunc(x[i]);
```
in questo caso assegnamo una iterazione per thread e poi si ricomincia. visto che è array sono contigue quindi stanno sulla stessa linea di memoria. quindi 16 elementi (considerando 4 byte per elemento e 64 byte di cache x line) si trovano su una linea di cache

Padded:
```c
double x[N][8];
#pragma omp parallel for schedue(static, 1)
for (int i=0; i<N; i++)
	x[i][0] = someFunc(x[i][0])
```

il modo in cui viene fatto è dunque (8 perchè 8 byte per double) trasformarlo in una matrice. in questo modo ogni elemento va su una linea di cache diversa ma ho moltiplicato l’utilizzo della memoria per 8

EVITARE PADDING SU ARRAY

in questo caso invece avrei potuto usare il data mapping change
### Data mapping change
```c
double x[N];
#pragma omp parallel for schedule(static, 8)
for (int i=0; i<N; i++)
	x[i] = someFunc(x[i])
```

in questo caso non ci sta false sharing, infatti il thread 0 accede ad 8 thread consecutivi. ogni thread accede soltanto alla propria linea di cache. funziona soltanto se l’array è inizializzato ad un multiplo di 64 (è importante assicurarsene)\

## Esempi per l’utilizzo delle cache e false sharing
### Pthreads matrix-vector multiplication
```c
void *Pth_mat_vect(void* rank) {
    long my_rank = (long) rank;
    int i, j;
    int local_m = m/thread_count;
    int my_first_row = my_rank*local_m;
    int my_last_row = (my_rank+1)*local_m - 1;
	
	for (i = my_first_row; i <= my_last_row; i++) {
		y[i] = 0.0;
	    for (j = 0; j < n; j++)
         y[i] += A[i][j]*x[j];
    }
	
    return NULL;
}  /* Pth_mat_vect */
```

supponiamo di fare la moltiplicazione con matrici e vettori di dimensione diversa. il numero totale di elementi è uguale ma cambia la forma


numero colonne x numero righe
![[Pasted image 20251209180621.png]]
