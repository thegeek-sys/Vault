# Parallel Design Patterns
## Program structure patterns
We can distinguish the parallel program structure patterns into two major categories:
- **Globally Parallel, Locally Sequential** (*GPLS*) → this means that the application is able to perform multiple tasks concurrently, with each task running sequentially. Applicazione di per sè è parallela ma ciascun processo all’interno esegue codice sequenziale. Patterns that fall in this category include:
	- Single-Program, Multiple Data
	- Multiple-Program, Multiple Data
	- Master-Worker → un processo coordinatore e smista le cose da eseguire ai processi worker
	- Map-reduce → fase map e fase di reduce, utilizzato su stream di array (supponiamo di voler calcolare  la somma tra tutti gli elementi dell’array, la fase di  map divide l’array ai vari processi e il reduce è l’operazione di somma eseguita dai singoli processi)
- **Globally Sequential, Locally Parallel** (*GSLP*) → this means that the application executes as a sequential program, with individual parts of it running in parallel when requested.  Patterns that fall in this category include:
	- fork/join
	- loop parallelism → durante un codice sequenziale, se so che posso eseguire il loop in parallelo, viene fatto

![[Pasted image 20251021173645.png]]

###  Globally Parallel, Locally Sequential
#### Single-Program, Multiple Data
Keeps all the application logica in a single program
Typical program structure involves:
- program initialization (eg. runtime initialization)
- obtaining a unique identifier → identifiers are numbered from $0$, enumerating the threads or process used. Some systems use vector identifiers (eg. CUDA)
- running the program → execution path diversified based on ID
- shutting down the program → clean-up, saving results, etc.

#### Multiple-Program, Multiple Data
SPMD fails when:

tutto ciò che sono i dati sono condivisi da tutti i processi (diversi eseguibili che lavorano su dati diversi)
non può girare sia su CPU che su GPU

#### Master-Worker
Two kind of components: Master and Workers
Master (one or more) is responsible for:
- handing out pieces of work to workers
- collecting the results of the computations from the workers
- performing I/O duties on behalf of the workers, (ig. sending them the data that they are supposed to process, or accessing a file)
- interacting with the user

It is good for implicit load balancing (no/few inter-worked data exchange) ogni volta che un worker finisce gli viene assegnato altro lavoro
Questa coda comporta che il master è un collo di bottiglia sul master (che divide il lavoro) per questo spesso si ha una gerarchia di master, così da poter evitare di avere un solo punto di fallimento
#### Map-reduce
It’s a variation of master-worker pattern and it’s an old concept, made popular by Google’s search engine

  il tipo di operazione è molto specifico map o reduce
  - map → apply a function on data, resulting in a set of partial results
  - reduce → collect the partial results and derive the complete one

Map and reduce workers can vary in number

>[!info] Master-worker vs. map-reduce
>- master-worker → same function applied to different data items
>- map-reduce → same function applied to different parts of a single data item (data parallel)

## Globally Sequential, Locally Parallel
### Fork/join
Single parent thread of execution and the children are created dynamically at run-time (can be slow). al posto di crearli/distruggerli li metto idle

Tasks may run via spawning of threads, or via use of a static pool of threads 

>[!example]
>```c
>mergesort(A, lo, hi):
>	if lo < hi:                    // at least one element of input
>		mid = floor(lo+(hi-lo)/2)
>		fork mergesort(A, lo, mid) // process (potentially) in parallel
>								   // parallel with main task
>		mergesort(A, mid, hi)      // main task handles second recursion
>		join
>		merges(A, lo, mid, hi)
>```

### Loop parallelism
se ho ciclo da 10 volte eseguo 10 thread nei quali ognuno esegue un ciclo

Employed for migration of legacy/sequential software to multicore. Focuses on breaking up loops by manipulating the loop control variable (but a loop has to be in a particular form to support this)

It has limited flexibility, but limited development effort as well and is supported by OpenMP


local_n numero di trapezi su cui ogni processo deve lavorare
local_a punto da dove devo iniziare a lavorare
local_b punto fino a dove devo lavorare

il processo 0 deve fare la somma


>[!error]
>Non è possibile inviare puntatori tramite MPI (ha senso solo per il processo che invia)

## Issues with the trapezoidal rule implementation
When doing the global sum, $p-1$ processes send their data to one process, which then computes all the sums.
Unbalance! How long does it take?
- for process $0$ → $(p-1)\cdot(T_{\text{sum}}+T_{\text{recv}})$
- for all other processes → $T_{\text{send}}$

Alternative
- for process $0$ → $\log_{2}(p)\cdot (T_{\text{sum}}+T_{\text{recv}})$

ancora sbilanciato ma ben meglio di prima

The optimal way to compute a global sum depends on the number of processes, the size of the data, and the system we are running on (how many NICs, how the nodes are connected, etc.)
Having a native way to express the global sum would simplify programming and improve performance. In MPI sono le comunicazioni collettive

## Collective communication 
### $\verb|MPI_Reduce|$
![[Pasted image 20251021183954.png]]

```c
int MPI_Reduce(
	void*        input_data_p,   // in
	void*        output_data_p,  // out
	int          count,          // in
	MPI_Datatype datatype,       // in
	MPI_Op       operator,       // in
	int          dest_process,   // in
	MPI_Comm     comm            // in
);
```

```c
MPI_Reduce(&local_int, &total_int, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD)
```

tutti i processi devono fare la reduce con gli stessi parametri

