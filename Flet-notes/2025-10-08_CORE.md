Due send fatte a uno stesso processo, queste arriveranno al ricevente allo stesso ordine, nonostante la rete possa riordinarli


| MPI datatype         | C datatype             |
| -------------------- | ---------------------- |
| `MPI_CHAR`           | `signed char`          |
| `MPI_SHORT`          | `signed short int`     |
| `MPI_LONG`           | `signed long int`      |
| `MPI_LONG_LONG`      | `signed long long int` |
| `MPI_UNSIGNED_CHAR`  | `unsigned char`        |
| `MPI_UNSIGNED_SHORT` | `unsigned short int`   |
| `MPI_UNSIGNED`       | `unsigned int`         |
| `MPI_UNSIGNED_LONG`  | `unsigned long int`    |
| `MPI_FLOAT`          | `float`                |
| `MPI_DOUBLE`         | `double`               |
| `MPI_LONG_DOUBLE`    | `long double`          |
| `MPI_BYTE`           |                        |
| `MPI_PACKED`         |                        |

## Message matching
Message is successfully received if:
- `recv_type = send_type`
- `recv_buf_sz >= send_buf_sz`

A receiver can get a mesasge without knowing:
- the amount of data in the message
- the sender of the message (`MPI_ANY_SOURCE`)
- or the tag of the message (`MPI_ANY_TAG`)

## $\verb|status_p|$ argument

```c
MPI_Recv(recv_buf_p, rect_buf_sz, recv_type, src, recv_tag, recv_comm, &status);
```

```c
int MPI_Get_count(
	MPI_Status*  status_p, // in
	MPI_Datatype type,     // in
	int*         count_p   // out
);
```

>[!error] Issues with send and receive
>- exact behavior is determined by the MPI implementation
>- `MPI_Send` may behave differently with regard to buffer size cutoffs and blocking. quando faccio la send non so se il messaggio sia ancora partito o meno, la mia unica garanzia è che quando eseguo l’istruzione successiva possono essere modificati dato che MPI li ha già copiati su un altro buffer di memoria (sicuramente però gli elementi che sono stati inviati sono quelli prima della modifica)
>- `MPI_Recv` always blocks until a matching message is received
>- even if you know your MPI implementation, stick to what is defined by the standard (don’t assume that the send returns immediately for small buffers), otherwise your code will not be portable

>[!warning] Warnings
>- if a process tries to receive a message and there’s no matching send, then the process will hang
>- if a call to `MPI_Send` blocks and there’s no matching receive, then the sending process can hang
>- if, a call to `MPI_Send` if buffered and there’s no matching reveice then the message will be lost
>- if the rank of the destination process is the same as the rank of the source process, a process will hand, or, perhaps worse, the receive may match another send

## What happens when you do a $\verb|Send|$
![[Pasted image 20251008145432.png]]

## Point-to-point communication modes
`MPI_Send` uses the so called standard communication mode. MPI decides based on the size of the message, whether to block the call until the destination process collects it or to return before a matching receive is issued. The latter is chosen if the message is small enough, making `MPI_Send` locally blocking

There are three additional communication modes:
- **buffered** → in buffered mode the sending operation is always locally blocking (eg. it will return as soon as the message is copied to a buffer). The second difference with the standard communication mode is that the buffer is *user-provided*
- **synchronous** → in synchronous mode, the sending operation will return only after the destination process has initiated and started the retrieval of the message. This is a proper **globally blocking** operation
- **ready** → the send operation will succeed only if a matching receive operation has been initiated already. Otherwise the function returns with an error code. The purpose of this mode is to reduce overhead of handshaking operations

```c
int [MPI_Bsend | MPI_Ssend | ]
```

## Non-blocking communication
Le chiamate non bloccanti hanno associato un comando di attesa, dato che non so se posso modificare il buffer dato che non viene aspettata la copia (non posso modificare il buffer se non so per certo che è stato già inviato)

una volta fatta la isend posso continuare a fare  altri calcoli (viene aperto in parallelo un thread che si occupa di di fare la send) e poi utilizzare il comando di wait che mi permette di aspettare finchè il buffer non è più in carico della isend (lo posso modificare). stessa cosa vale per la receive

>[!example] Example
>**Problem** → ring: each rank sends something to left/right rank, and receives something from them
>
>![[Pasted image 20251008162902.png]]
>
>Mi è utile utilizzare il non bloccante poichè potrebbero esserci problemi di deadlock (se due core fanno send allo stesso tempo con messaggi sufficientemente grandi, si ha un deadlock)

```c
#include "mpi.h"
#include <stdio.h>

int main(void) {
	int numtasks, rank, next, prev, buf[2];
	MPI_Request reqs[4]; // required variable for non-blocking calls
	MPI_Status stats[4]; // required variable for Waitall routine
	MPI_Init(NULL, NULL);
	MPI_Comm_size(MPI_COMM_WORLD, &numtasks);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank); // determine left and right neighbors
	prev = (rank-1+numtask) % numtasks;
	next = (rank+1) % numtasks;
// post non-blocking receives and sends for neighbors
	MPI_Irecv(&buf[0], 1, MPI_INT, prev, 0, MPI_COMM_WORLD, &reqs[0]);
	MPI_Irecv(&buf[1], 1, MPI_INT, next, 0, MPI_COMM_WORLD, &reqs[1]);
	MPI_Isend(&rank, 1, MPI_INT, prev, 0, MPI_COMM_WORLD, &reqs[2]);
	MPI_Isend(&rank, 1, MPI_INT, next, 0, MPI_COMM_WORLD, &reqs[3]);
	// do some work while sends/receives progress in background
	// wait for all non-blocking operations to complete
	MPI_Waitall(4, reqs, stats);
	// continue - do more work
	MPI_Finalize();
}
```