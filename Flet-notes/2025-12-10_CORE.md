## Shared memory
On-chip memory

shared memory è condivisa dai core in esecuzione su quell’sm

Can be used as:
- a place to hold frequently used data that would otherwise require a global memory access
- as a way for cores on the same SM to share data

The `__shared__` specifier can be used to indicate that some data must go in the shared on-chip memory rather than on the global memory

>[!info] Shared memory vs. L1 cache
>- both are on-chip. The former is managed by the programmer the latter automatically
>- in some cases, managing it manually (i.e., using the shared memory), might provide better performance (e.g., you do not have any guarantee that the data you need will be in the L1 cache, but with the explicitely managed shared memory, you can control that)

>[!example] 1D stencil
>viene aggiornato in base al valore dei vicini (valore dell’elemento viene aggiornato in base al valore dei vicini)
>
>Considera applying a 1D stencil to a 1D array of elements (each output element is the sum of input elements within a radius)
>
>![[Pasted image 20251210145255.png|350]]
>If radius is 3, then each output element is the sum of 7 input elements
>
>Each thread processes one output element (`blockDim.x` elements per block), so input elements are read several times. For this reason with radius 3, each input element is read seven times

```c
__global__ void stencil_1d(int *in, int *out) {
	// 2*RADIUS padding all'inizio e alla fine
	__shared__ int temp[BLOCK_SIZE + 2 * RADIUS];
	int gindex = threadIdx.x + blockIdx.x * blockDim.x;
	// in posizione threadId.x non sto considerando halo, devo aggiungere radius
	int lindex = threadIdx.x + RADIUS;
	
	// read input elements into shared memory
	temp[index] = in[gindex];
	if (threadIdx.x < RADIUS) {
		temp[lindex-RADIUS] = in[gindex-RADIUS];
		temp[lindex+BLOCK_SIZE] = in[gindex+BLOCK_SIZE]
	}
	
	__syncthreads();
	
	// apply the stencil
	int result = 0;
	for (int offset=-RADIUS; offset<=RADIUS; offset++)
		result += temp[lindex+offset];
	
	// store the result
	out[gindex] = result;
}
```

non hogaranzia sull’ordine del warp. quindi bisogna garantire che prima del for siano tutti sincronizzati. in particolare potremmo avere problemi in caso in cui il lindex+RADIUS contiene l’halo (thread vuole elemento 32 ma thread che doveva caricare il 32 non è stato ancora eseguito)

per questo ho inserito una `__syncthreads()`

## Constant memory
solo in lettura, mai aggiornati but constant memory is not ROM. It is just a memory that can hold constant values (i.e., the host can write it)

There are two main advantages:
- it is cached
- supports broadcasting of a single value to all threads in a warp

>[!example]
>Suppose to have 10 warps on an SM, and all request the same variable:
>- if data is on global memory
>	- all warp will request the same segment from global memory
>	- the first time segment is copied into L2 cache
>	- if other data pass through L2, there are good chances it will be lost
>	- there are good chances that data should be requested multiple times
>- if data is in constant memory
>	- during first warp request, data is copied in constant-cache
>	- since there is less traffic in constant-cache, there are good chances all other warp will find the data already in cache

```c
__constant__ type variable_name; // static
cudaMemcpyToSymbol(variable_name, &host_src, sizeof(type), cudaMemcpyHostToDevice);
// warning: cannot be dynamically allocated
```

- data will reside in the constant memory address space
- had static storage duration (persists until the application ends)
- readable from all threads of a kernel

## Image to grayscale
An image can be seen as a 2D matrix of pixels, each pixel has 3 values for the 3 RGB channels. Number of columns = 3 x Number of pixels in a row

ogni pixel ha tre numeri a virgola mobile per r, g, b

greyOffset indice dell’elemento su cui devo lavorare nell’array linerarizzata (tutto parallelizzato, indice nell’immagine linearizzata)
CHANNELS numero di canali (=3, rgb)

## Image blur kernel
simile a stencil

## Performance estimation

>[!question] How can we measure performance to have an idea of whether we are saturating the computational capabilities of the hardware?
>We use $\text{FLOP/s}$ (i.e., floating-point operations per second)
>
>>[!question] What type of floating-point? 64-bit, 32-bit, 16-bit, …?

Today we have systems capable of 1 ExaFLOP/s (i.e., $10^{18} \text{FLOP/s}$)

>[!warning]
>Be aware that different books/resources might denote it differently

>[!example]
>```c
>pixVal += in[cureRow * w + curCol];
>```
>
>All threads access global memory for their input matrix elements. Let’s suppose that the global memory bandwidth is $200\text{ GB/s}$
>
>>[!question] How many operands can we load?
>>$$\frac{200\text{ GB/s}}{4\text{ bytes}}=50G \text{operands/s}$$
>
>We do one floating-point operation (`+=`) on each operand. Thus, we can expect, in the best case, a peak performance of $50\text{ GFLOP/s}$
>
>Let’s suppose that the peak floating-point rate of this GPU is $1500\text{ GFLOP/s}$
>
>This limits the execution rate to $3.3\%$ of the peak floating-point execution rate of the device (i.e., the memory movement to/from the memory, rather than the compute capacity, is limiting our performance)

We define the **compute-to-global-memory access ratio** as the number of floating-point calculation performed for each access to the global memory within a region of a program

per raggiungere 1.5 teraflops su ogni operando bisogna fare almeno 30 operazioni (per ammortizzare l’accesso a memoria)