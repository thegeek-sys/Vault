## Shared memory
On-chip memory

shared memory è condivisa dai core in esecuzione su quell’sm

Can be used as:
- a place to hold frequently used data that would otherwise require a global memory access
- as a way for cores on the same SM to share data

The `__shared__` specifier can be used to indicate that some data must go in the shared on-chip memory rather than on the global memory

>[!info] Shared memory vs. L1 cache
>- both are on-chip. The former is managed by the programmer the latter automatically
>- in some cases, managing it manually (i.e., using the shared memory), might provide better performance (e.g., you do not have any guarantee that the data you need will be in the L1 cache, but with the explicitely managed shared memory, you can control that)

>[!example] 1D stencil
>viene aggiornato in base al valore dei vicini (valore dell’elemento viene aggiornato in base al valore dei vicini)
>
>Considera applying a 1D stencil to a 1D array of elements (each output element is the sum of input elements within a radius)
>
>![[Pasted image 20251210145255.png|350]]
>If radius is 3, then each output element is the sum of 7 input elements
>
>Each thread processes one output element (`blockDim.x` elements per block), so input elements are read several times. For this reason with radius 3, each input element is read seven times

```c
__global__ void stencil_1d(int *in, int *out) {
	// 2*RADIUS padding all'inizio e alla fine
	__shared__ int temp[BLOCK_SIZE + 2 * RADIUS];
	int gindex = threadIdx.x + blockIdx.x * blockDim.x;
	// in posizione threadId.x non sto considerando halo, devo aggiungere radius
	int lindex = threadIdx.x + RADIUS;
	
	// read input elements into shared memory
	temp[index] = in[gindex];
	if (threadIdx.x < RADIUS) {
		temp[lindex-RADIUS] = in[gindex-RADIUS];
		temp[lindex+BLOCK_SIZE] = in[gindex+BLOCK_SIZE]
	}
	
	__syncthreads();
	
	// apply the stencil
	int result = 0;
	for (int offset=-RADIUS; offset<=RADIUS; offset++)
		result += temp[lindex+offset];
	
	// store the result
	out[gindex] = result;
}
```

non hogaranzia sull’ordine del warp. quindi bisogna garantire che prima del for siano tutti sincronizzati. in particolare potremmo avere problemi in caso in cui il lindex+RADIUS contiene l’halo (thread vuole elemento 32 ma thread che doveva caricare il 32 non è stato ancora eseguito)

per questo ho inserito una `__syncthreads()`

## Constant memory
solo in lettura, mai aggiornati but constant memory is not ROM. It is just a memory that can hold constant values (i.e., the host can write it)

There are two main advantages:
- it is cached
- supports broadcasting of a single value to all threads in a warp

>[!example]
>Suppose to have 10 warps on an SM, and all request the same variable:
>- if data is on global memory
>	- all warp will request the same segment from global memory
>	- the first time segment is copied into L2 cache
>	- if other data pass through L2, there are good chances it will be lost
>	- there are good chances that data should be requested multiple times
>- if data is in constant memory
>	- during first warp request, data is copied in constant-cache
>	- since there is less traffic in constant-cache, there are good chances all other warp will find the data already in cache

```c
__constant__ type variable_name; // static
cudaMemcpyToSymbol(variable_name, &host_src, sizeof(type), cudaMemcpyHostToDevice);
// warning: cannot be dynamically allocated
```

- data will reside in the constant memory address space
- had static storage duration (persists until the application ends)
- readable from all threads of a kernel

## Image to grayscale
An image can be seen as a 2D matrix of pixels, each pixel has 3 values for the 3 RGB channels. Number of columns = 3 x Number of pixels in a row

ogni pixel ha tre numeri a virgola mobile per r, g, b

greyOffset indice dell’elemento su cui devo lavorare nell’array linerarizzata (tutto parallelizzato, indice nell’immagine linearizzata)
CHANNELS numero di canali (=3, rgb)

## Image blur kernel
simile a stencil
