## CPU: latency oriented design
High clock frequency
Large caches
- convert long latency memory access to short latency cache accesses
Sophisticated control
- branch prediction for reduced branch latency
- out-of-order exectution
- etrc
Powerful ALU (where the computation is done)
- reduced operawtion latency

![[Pasted image 20251125171918.png]]

GPU progettata per ridurra il più possibile il tempo di esecuzione delle istruzioni

## GPUs: throughput oriented design
Moderate clock frequency
Small caches
simple control 
- no branch prediction
- in-order execution
Energy efficient ALUs
- many, long latency but heavily pipelined for high throughput
Require massive member of threads to tollerate latencies
Use high bandwidth interfaces for memory and host data exchange

![[Pasted image 20251125172404.png]]

latenza (quanto tempo ci emtto ad esegue un task) throughput (numero di task eseguite per unità di tempo)

unità di controllo e cache molto piccole. le unità di controllo sono molto più semplcici quindi molte più unità di esecuzione

avendo così tanti core è possibile mascherare gli accessi alla memoria (mentre si spostano i dati è possibile eseguire altre cose)

Quando più è complessa una ALU più è grande l’area che occupa all’interno della GPU

il tempo di accesso alla sua memoria è molto più veloce (la CPU usa la dram mentre la GPU ne ha una dedicata)

## Architecture of a CUDA-capable GPU
![[Pasted image 20251125172610.png]]

## Application benefits from both CPU and GPU
CPUs for sequential parts where latency mattes (CPUs can be 10+X faster than GPUs for sequantial code)

GPUs for parallel parts where throughput matters (GPUs cab be 10+X faster than CPUs for parallel code)

### CPU-GPU architecture
![[Pasted image 20251125172957.png]]

## GPU programming caveats
GPU program deployment has a characteristic that can be considered a major obstacle: GPU and host memories are typically disjoint, requiring explicit (or implicit, depending on the development platform) data transfer between the two

A second characteristic of GPU computation is that GPU ….

## GPU software development platforms
CUDA → compute unified device architecture. CUDA provides two sets of APIs (a low and a higher level one) and it is available freely for WIndows, MacOS X and Linux operating systems. Major drawback: NVidia hardware only (even though now there are tools to run CUDA code on AMD GPUs)

HPI → AMD’s equivalent of CUDA. Tools provided to convert CUDA to HIP code (HIPIFY)

OpenCL → open computing language is an open standard for writing programs that can execute across a variety of heterogeneous platforms that include GPUs. CPU, DSPs or other processor. OpenCL is supported by both NVidia and AMD. It is the primary development plaform for AMD GPUs. OpenCL’s programming model matched closely the one offered by CUDA

OpenACC → an open specification for an API that allows the use of compiler directives (e.g. `#pragma acc` )   ….

## CUDA
It enables a general-purpose programming model on NVIDIA GPUs: before CUDA, GPU was programmed by transofrming an algorithm in a sequence of image manipulation primitives

Enables explicit GPU memory management. The GPU is viewed as a compute device that:
- is a co-processor to the CPU (or host)
- has its own DRAM (global memory in CUDA parlance)
- runs many threads in parallel: thread creation/switching cost is few clock cycles

>[!warning]
>CUDA is a platform/programming model, not a programming language

## CUDA program structure
Steps:
- allocate GPU memory
- transfer data from host to GPU memory
- run CUDA kernel → funzione che viene eseguita sulla GPU
- copy result from GPU memory to host memory → per poi farci qualsiasi cosa lo devo rimettere nella memoria host

## CUDA execution model
![[Pasted image 20251125174829.png]]

In the vast majority of scenarios, the host is responsible for I/O operations, passing the input and subsequently collecting the output data from the memory space of the GPU

![[Pasted image 20251125174939.png]]

CUDA organized threads in a 6D structure (lower dimensions are also possible). Each thread has a position in a 1D, 2D or 3D *block*. Each block has a position in a 1D, 2D, or 3D *grid*. Each thread is aware of its position in the overall structure, via a set of intrinsic variables/structures. With this information a thread can map its position to the subset od data that is assigned to.

In the picture each block is 2D, while the grids are 3D. Scegliamo la struttura in base a su cosa sto lavorando (es. immagine grid 2D block 1D, fluido 3D, …)

## CUDA compute capability
The sizes of blocks and grids are determined by the capability, which determined what each generation of GPUs is capable of

The compute capability of a device is represented by a version number, also sometimes called its “SM version”

This version number identifies the feature supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU

![[Pasted image 20251125175932.png]]

![[Pasted image 20251125175959.png]]

## How to write a program?
You must specify a function that is going to be executed by all the threads (SIMD/SPMD/SIMT). This function is called **kernel**

You must specify how threads are arranged in the grid/blocks

```c
// (immagine di prima)
dim3 block(3,2); // non specificando le altre dimensio da per scontato 1
dim3 grid(4,3,2)

foo<<<grid. block>>>(); // attiva al funzione eseguita sulla GPU
```

- `dim3` is a vector of `int`
- every non-specified component is set to $1$
- every component accessible to $x$, $y$, $z$ fields (will se it later)

### How to specify grid/block size

```c
dim3 b(3,3,3);
dim3 g(20,100);
foo<<<g. b>>>(); // run a 20x100 grid of 3x3x3 blocks
foo<<<10, b>>>(); // run a 10 block grid, each made by 3x3x3 threads
foo<<<g, 256>>>(); //
foo<<<g, 2048>>>(); //
foo<<<5, g>>>(); //
```

## Hello world in CUDA

```c
// file hello.cu
#include <stdio.h>
#include <cuda.h>

// can be called from the host or the device. must run on the device (GPU)
// it's a decorator
// a kernel is always declared as void. computed result must be copied explicitly from GPU to host memory
__global__ void hello() {
	// supported by CC 2.0 (compute capability)
	printf("Hello world!\n");
}

int main() {
	hello<<<1,10>>>();
	// blocks until the CUDA kernel terminates
	cudaDeviceSyncronize();
	return 1;
}
```

To compile and run:
```bash
# arch specifica la capability della GPU
$ nvcc --arch=sm_20 hello.cu -o hello
$ ./hello
```

>[!warning]
>`printf` is detrimental for performance (GPU should not do I/O). Only use it for debugging purposes

## Function decoration
- `__global__` → can be called from the host or the GPU and executed on the device/GPU. In CC 3.5 and above, the device can also call `__global__` functions
- `__device__` → a function that runs on the GPU and can only be called from within a kernel (i.e. from the GPU)
- `__host__` → a function that can only run on the host. The `__host__` qualifier is typically omitted, unless used in combination with `__device__` to indicate that the function can run on both the host and the device. Such scenario implies the generation of two compiled codes for the function

## How to get the thread position in the grid/block

>[!info]
>Remember that threads are arranged in a 6D space

>[!example]
>![[Pasted image 20251125182427.png]]
>
>threadIdx (posizione assoluta non considerando la divisione in blocchi e grid)
>$$x=\verb|blockIdx.x|\times \verb|blockDim.x|+\verb|threadIdx.x|=1\times 4+3=7$$
>$$y=\verb|blockIdx.y|\times \verb|blockDim.y|+\verb|threadIdx.y|=2\times 4+1=9$$
>$$\verb|threadId|=9\times 16+7$$
>
>![[Pasted image 20251125183226.png]]

## Thread scheduling
Each thread runs on a streaming processor (CUDA core)
Sets of cores on the same SM share the control unit (i.e. they must synchronously execute the same instruction)
Different SMs can run different kernels
Each block runs on an SM (i.e. I can’t have a block spanning over multiple SMs, but I can have more blocks running on the same SM)
Once a block is fully executed, the SM will run the next one
Not all the threads in a block run concurrently

un blocco non può runnare su più sm

## Warps
They are executed in groups called **warps** (in current GPUs, the size of warp is 32)
Thread in a block are split into warps according to their intra-block ID (i.e. the first 32 threads in a block belong to the same )

warp divergence fa si che venga ridotta l’utilizzo della GPU