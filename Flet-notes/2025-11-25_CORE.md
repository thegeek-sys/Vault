## CPU: latency oriented design
High clock frequency
Large caches
- convert long latency memory access to short latency cache accesses
Sophisticated control
- branch prediction for reduced branch latency
- out-of-order exectution
- etrc
Powerful ALU (where the computation is done)
- reduced operawtion latency

![[Pasted image 20251125171918.png]]

GPU progettata per ridurra il più possibile il tempo di esecuzione delle istruzioni

## GPUs: throughput oriented design
Moderate clock frequency
Small caches
simple control 
- no branch prediction
- in-order execution
Energy efficient ALUs
- many, long latency but heavily pipelined for high throughput
Require massive member of threads to tollerate latencies
Use high bandwidth interfaces for memory and host data exchange

![[Pasted image 20251125172404.png]]

latenza (quanto tempo ci emtto ad esegue un task) throughput (numero di task eseguite per unità di tempo)

unità di controllo e cache molto piccole. le unità di controllo sono molto più semplcici quindi molte più unità di esecuzione

avendo così tanti core è possibile mascherare gli accessi alla memoria (mentre si spostano i dati è possibile eseguire altre cose)

Quando più è complessa una ALU più è grande l’area che occupa all’interno della GPU

il tempo di accesso alla sua memoria è molto più veloce (la CPU usa la dram mentre la GPU ne ha una dedicata)

## Architecture of a CUDA-capable GPU
![[Pasted image 20251125172610.png]]

## Application benefits from both CPU and GPU
CPUs for sequential parts where latency mattes (CPUs can be 10+X faster than GPUs for sequantial code)

GPUs for parallel parts where throughput matters (GPUs cab be 10+X faster than CPUs for parallel code)

### CPU-GPU architecture
![[Pasted image 20251125172957.png]]

