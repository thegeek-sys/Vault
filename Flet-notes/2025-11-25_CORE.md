## CPU: latency oriented design
High clock frequency
Large caches
- convert long latency memory access to short latency cache accesses
Sophisticated control
- branch prediction for reduced branch latency
- out-of-order exectution
- etrc
Powerful ALU (where the computation is done)
- reduced operawtion latency

![[Pasted image 20251125171918.png]]

GPU progettata per ridurra il più possibile il tempo di esecuzione delle istruzioni

## GPUs: throughput oriented design
Moderate clock frequency
Small caches
simple control 
- no branch prediction
- in-order execution
Energy efficient ALUs
- many, long latency but heavily pipelined for high throughput
Require massive member of threads to tollerate latencies
Use high bandwidth interfaces for memory and host data exchange

![[Pasted image 20251125172404.png]]

latenza (quanto tempo ci emtto ad esegue un task) throughput (numero di task eseguite per unità di tempo)

unità di controllo e cache molto piccole. le unità di controllo sono molto più semplcici quindi molte più unità di esecuzione

avendo così tanti core è possibile mascherare gli accessi alla memoria (mentre si spostano i dati è possibile eseguire altre cose)

Quando più è complessa una ALU più è grande l’area che occupa all’interno della GPU

il tempo di accesso alla sua memoria è molto più veloce (la CPU usa la dram mentre la GPU ne ha una dedicata)

## Architecture of a CUDA-capable GPU
![[Pasted image 20251125172610.png]]

## Application benefits from both CPU and GPU
CPUs for sequential parts where latency mattes (CPUs can be 10+X faster than GPUs for sequantial code)

GPUs for parallel parts where throughput matters (GPUs cab be 10+X faster than CPUs for parallel code)

### CPU-GPU architecture
![[Pasted image 20251125172957.png]]

## GPU programming caveats
GPU program deployment has a characteristic that can be considered a major obstacle: GPU and host memories are typically disjoint, requiring explicit (or implicit, depending on the development platform) data transfer between the two

A second characteristic of GPU computation is that GPU ….

## GPU software development platforms
CUDA → compute unified device architecture. CUDA provides two sets of APIs (a low and a higher level one) and it is available freely for WIndows, MacOS X and Linux operating systems. Major drawback: NVidia hardware only (even though now there are tools to run CUDA code on AMD GPUs)

HPI → AMD’s equivalent of CUDA. Tools provided to convert CUDA to HIP code (HIPIFY)

OpenCL → open computing language is an open standard for writing programs that can execute across a variety of heterogeneous platforms that include GPUs. CPU, DSPs or other processor. OpenCL is supported by both NVidia and AMD. It is the primary development plaform for AMD GPUs. OpenCL’s programming model matched closely the one offered by CUDA

OpenACC → an open specification for an API that allows the use of compiler directives (e.g. `#pragma acc` )   ….

## CUDA
It enables a general-purpose programming model on NVIDIA GPUs: before CUDA, GPU was programmed by transofrming an algorithm in a sequence of image manipulation primitives

Enables explicit GPU memory management. The GPU is viewed as a compute device that:
- is a co-processor to the CPU (or host)
- has its own DRAM (global memory in CUDA parlance)
- runs many threads in parallel: thread creation/switching cost is few clock cycles

>[!warning]
>CUDA is a platform/programming model, not a programming language

## CUDA program structure
Steps:
- allocate GPU memory
- transfer data from host to GPU memory
- run CUDA kernel → funzione che viene eseguita sulla GPU
- copy result from GPU memory to host memory → per poi farci qualsiasi cosa lo devo rimettere nella memoria host

## CUDA execution model
![[Pasted image 20251125174829.png]]

In the vast majority of scenarios, the host is responsible for I/O operations, passing the input and subsequently collecting the output data from the memory space of the GPU

![[Pasted image 20251125174939.png]]

CUDA organized threads in a 6D structure (lower dimensions are also possible). Each thread has a position in a 1D, 2D or 3D *block*. Each block has a position in a 1D, 2D, or 3D *grid*. Each thread is aware of its position in the overall structure, via a set of intrinsic variables/structures. With this information a thread can map its position to the subset od data that is assigned to.

In the picture each block is 2D, while the grids are 3D. Scegliamo la struttura in base a su cosa sto lavorando (es. immagine grid 2D block 1D, fluido 3D, …)

## CUDA compute capability
The sizes of blocks and grids are determined by the capability, which determined what each generation of GPUs is capable of

The compute capability of a device is represented by a version number, also sometimes called its “SM version”

This version number identifies the feature supported by the GPU hardware and is used by applications at runtime to determine which hardware features and/or instructions are available on the present GPU

![[Pasted image 20251125175932.png]]

![[Pasted image 20251125175959.png]]